{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## SPAM detection task\nThe data contains 100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. The task is to make a spam detector. \ntrain.csv - contains 600 emails x 100 features for use training model(s)\ntrain_labels.csv - contains labels for the 600 training emails (1 = spam, 0 = normal)\ntest.csv - contains 4000 emails x 100 features. Need to detect the spam on them.\n\nPredictions can be continuous numbers or 0/1 labels. No header is necessary. Submissions are judged on area under the ROC curve. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Will import libraries\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize as sp\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import linear_model, model_selection, metrics, tree, ensemble ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading data\ndata = pd.read_csv('../input/just-the-basics-the-after-party/train.csv')\ndataT = pd.read_csv('../input/just-the-basics-the-after-party/test.csv')\ny = pd.read_csv('../input/just-the-basics-the-after-party/train_labels.csv')\ndata.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   0.097094   1.1133     45.038  0.88184  0.087009    1.041   1.5486    3.498  \\\n0  0.050086  0.11158    94.0800  1.76500  0.089417  4.80470  0.26742      NaN   \n1  0.088447  2.36340     5.0580  0.14436  0.064547  2.44400  4.25450  0.36506   \n2  0.772540  0.59469        NaN  0.97515  0.015987  0.52884  1.48840  3.96100   \n3  0.382410  4.81090  1955.1000  0.46050  0.024453  2.02980  3.74030  4.22810   \n4  0.081316  4.84150     4.0507  2.48320  0.058990  2.37940  1.61270  2.04220   \n\n    1.8578  0.0096729  ...  0.076209   3.6654  0.061607  0.0031605  0.036038  \\\n0  0.56473   0.035123  ...  0.054712  4.16870  0.075432   0.010869  0.063972   \n1  1.86090   0.009759  ...  0.017203  4.56130  0.046505        NaN  0.084066   \n2  4.80630   0.048617  ...  0.022891  0.12832  0.065028   0.036862  0.010010   \n3  2.42920   0.156830  ...  0.032051  4.37010  1.001100   0.065750  0.043547   \n4  1.65710   0.039377  ...  0.018918  2.68040  0.076524   0.082756  0.041953   \n\n     0.0845  2.4517  3.3373  0.065201  0.091158  \n0  0.079892  1.9795  3.5064  0.072132  0.091950  \n1  0.064829  3.3087  2.9969  0.064328  0.036793  \n2  0.020709  2.5237  2.1711  0.080865  0.081553  \n3  0.629430  4.6262  3.1947       NaN  0.187180  \n4  0.018092  3.3041  0.1922  0.032600  0.050172  \n\n[5 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.097094</th>\n      <th>1.1133</th>\n      <th>45.038</th>\n      <th>0.88184</th>\n      <th>0.087009</th>\n      <th>1.041</th>\n      <th>1.5486</th>\n      <th>3.498</th>\n      <th>1.8578</th>\n      <th>0.0096729</th>\n      <th>...</th>\n      <th>0.076209</th>\n      <th>3.6654</th>\n      <th>0.061607</th>\n      <th>0.0031605</th>\n      <th>0.036038</th>\n      <th>0.0845</th>\n      <th>2.4517</th>\n      <th>3.3373</th>\n      <th>0.065201</th>\n      <th>0.091158</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.050086</td>\n      <td>0.11158</td>\n      <td>94.0800</td>\n      <td>1.76500</td>\n      <td>0.089417</td>\n      <td>4.80470</td>\n      <td>0.26742</td>\n      <td>NaN</td>\n      <td>0.56473</td>\n      <td>0.035123</td>\n      <td>...</td>\n      <td>0.054712</td>\n      <td>4.16870</td>\n      <td>0.075432</td>\n      <td>0.010869</td>\n      <td>0.063972</td>\n      <td>0.079892</td>\n      <td>1.9795</td>\n      <td>3.5064</td>\n      <td>0.072132</td>\n      <td>0.091950</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.088447</td>\n      <td>2.36340</td>\n      <td>5.0580</td>\n      <td>0.14436</td>\n      <td>0.064547</td>\n      <td>2.44400</td>\n      <td>4.25450</td>\n      <td>0.36506</td>\n      <td>1.86090</td>\n      <td>0.009759</td>\n      <td>...</td>\n      <td>0.017203</td>\n      <td>4.56130</td>\n      <td>0.046505</td>\n      <td>NaN</td>\n      <td>0.084066</td>\n      <td>0.064829</td>\n      <td>3.3087</td>\n      <td>2.9969</td>\n      <td>0.064328</td>\n      <td>0.036793</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.772540</td>\n      <td>0.59469</td>\n      <td>NaN</td>\n      <td>0.97515</td>\n      <td>0.015987</td>\n      <td>0.52884</td>\n      <td>1.48840</td>\n      <td>3.96100</td>\n      <td>4.80630</td>\n      <td>0.048617</td>\n      <td>...</td>\n      <td>0.022891</td>\n      <td>0.12832</td>\n      <td>0.065028</td>\n      <td>0.036862</td>\n      <td>0.010010</td>\n      <td>0.020709</td>\n      <td>2.5237</td>\n      <td>2.1711</td>\n      <td>0.080865</td>\n      <td>0.081553</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.382410</td>\n      <td>4.81090</td>\n      <td>1955.1000</td>\n      <td>0.46050</td>\n      <td>0.024453</td>\n      <td>2.02980</td>\n      <td>3.74030</td>\n      <td>4.22810</td>\n      <td>2.42920</td>\n      <td>0.156830</td>\n      <td>...</td>\n      <td>0.032051</td>\n      <td>4.37010</td>\n      <td>1.001100</td>\n      <td>0.065750</td>\n      <td>0.043547</td>\n      <td>0.629430</td>\n      <td>4.6262</td>\n      <td>3.1947</td>\n      <td>NaN</td>\n      <td>0.187180</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.081316</td>\n      <td>4.84150</td>\n      <td>4.0507</td>\n      <td>2.48320</td>\n      <td>0.058990</td>\n      <td>2.37940</td>\n      <td>1.61270</td>\n      <td>2.04220</td>\n      <td>1.65710</td>\n      <td>0.039377</td>\n      <td>...</td>\n      <td>0.018918</td>\n      <td>2.68040</td>\n      <td>0.076524</td>\n      <td>0.082756</td>\n      <td>0.041953</td>\n      <td>0.018092</td>\n      <td>3.3041</td>\n      <td>0.1922</td>\n      <td>0.032600</td>\n      <td>0.050172</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 100 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the dataset has no headers, let's name the columns for further incrimination. \ncolums = list((range(0,100)))\ndata.columns = [colums]\ndataT.columns = [colums]\ndata.info()","execution_count":5,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 599 entries, 0 to 598\nData columns (total 100 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   (0,)    584 non-null    float64\n 1   (1,)    589 non-null    float64\n 2   (2,)    579 non-null    float64\n 3   (3,)    579 non-null    float64\n 4   (4,)    574 non-null    float64\n 5   (5,)    583 non-null    float64\n 6   (6,)    586 non-null    float64\n 7   (7,)    582 non-null    float64\n 8   (8,)    580 non-null    float64\n 9   (9,)    580 non-null    float64\n 10  (10,)   586 non-null    float64\n 11  (11,)   583 non-null    float64\n 12  (12,)   585 non-null    float64\n 13  (13,)   576 non-null    float64\n 14  (14,)   572 non-null    float64\n 15  (15,)   583 non-null    float64\n 16  (16,)   577 non-null    float64\n 17  (17,)   587 non-null    float64\n 18  (18,)   582 non-null    float64\n 19  (19,)   578 non-null    float64\n 20  (20,)   582 non-null    float64\n 21  (21,)   578 non-null    float64\n 22  (22,)   584 non-null    float64\n 23  (23,)   581 non-null    float64\n 24  (24,)   582 non-null    float64\n 25  (25,)   590 non-null    float64\n 26  (26,)   587 non-null    float64\n 27  (27,)   581 non-null    float64\n 28  (28,)   584 non-null    float64\n 29  (29,)   579 non-null    float64\n 30  (30,)   575 non-null    float64\n 31  (31,)   581 non-null    float64\n 32  (32,)   590 non-null    float64\n 33  (33,)   586 non-null    float64\n 34  (34,)   581 non-null    float64\n 35  (35,)   582 non-null    float64\n 36  (36,)   577 non-null    float64\n 37  (37,)   580 non-null    float64\n 38  (38,)   578 non-null    float64\n 39  (39,)   585 non-null    float64\n 40  (40,)   578 non-null    float64\n 41  (41,)   579 non-null    float64\n 42  (42,)   586 non-null    float64\n 43  (43,)   578 non-null    float64\n 44  (44,)   585 non-null    float64\n 45  (45,)   582 non-null    float64\n 46  (46,)   587 non-null    float64\n 47  (47,)   585 non-null    float64\n 48  (48,)   584 non-null    float64\n 49  (49,)   587 non-null    float64\n 50  (50,)   584 non-null    float64\n 51  (51,)   579 non-null    float64\n 52  (52,)   576 non-null    float64\n 53  (53,)   586 non-null    float64\n 54  (54,)   576 non-null    float64\n 55  (55,)   578 non-null    float64\n 56  (56,)   587 non-null    float64\n 57  (57,)   578 non-null    float64\n 58  (58,)   583 non-null    float64\n 59  (59,)   587 non-null    float64\n 60  (60,)   578 non-null    float64\n 61  (61,)   587 non-null    float64\n 62  (62,)   586 non-null    float64\n 63  (63,)   586 non-null    float64\n 64  (64,)   579 non-null    float64\n 65  (65,)   576 non-null    float64\n 66  (66,)   579 non-null    float64\n 67  (67,)   577 non-null    float64\n 68  (68,)   584 non-null    float64\n 69  (69,)   581 non-null    float64\n 70  (70,)   577 non-null    float64\n 71  (71,)   586 non-null    float64\n 72  (72,)   588 non-null    float64\n 73  (73,)   586 non-null    float64\n 74  (74,)   580 non-null    float64\n 75  (75,)   578 non-null    float64\n 76  (76,)   581 non-null    float64\n 77  (77,)   581 non-null    float64\n 78  (78,)   577 non-null    float64\n 79  (79,)   587 non-null    float64\n 80  (80,)   584 non-null    float64\n 81  (81,)   577 non-null    float64\n 82  (82,)   581 non-null    float64\n 83  (83,)   582 non-null    float64\n 84  (84,)   577 non-null    float64\n 85  (85,)   580 non-null    float64\n 86  (86,)   581 non-null    float64\n 87  (87,)   586 non-null    float64\n 88  (88,)   587 non-null    float64\n 89  (89,)   585 non-null    float64\n 90  (90,)   578 non-null    float64\n 91  (91,)   584 non-null    float64\n 92  (92,)   583 non-null    float64\n 93  (93,)   577 non-null    float64\n 94  (94,)   570 non-null    float64\n 95  (95,)   582 non-null    float64\n 96  (96,)   583 non-null    float64\n 97  (97,)   581 non-null    float64\n 98  (98,)   580 non-null    float64\n 99  (99,)   573 non-null    float64\ndtypes: float64(100)\nmemory usage: 468.1 KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And let's fill in the missing values with the median\nfor i in colums:\n    data[i,].fillna(data[i,].median(), inplace = True)\n\nfor i in colums:\n    dataT[i,].fillna(dataT[i,].median(), inplace = True)\ndata.info()","execution_count":6,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 599 entries, 0 to 598\nData columns (total 100 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   (0,)    599 non-null    float64\n 1   (1,)    599 non-null    float64\n 2   (2,)    599 non-null    float64\n 3   (3,)    599 non-null    float64\n 4   (4,)    599 non-null    float64\n 5   (5,)    599 non-null    float64\n 6   (6,)    599 non-null    float64\n 7   (7,)    599 non-null    float64\n 8   (8,)    599 non-null    float64\n 9   (9,)    599 non-null    float64\n 10  (10,)   599 non-null    float64\n 11  (11,)   599 non-null    float64\n 12  (12,)   599 non-null    float64\n 13  (13,)   599 non-null    float64\n 14  (14,)   599 non-null    float64\n 15  (15,)   599 non-null    float64\n 16  (16,)   599 non-null    float64\n 17  (17,)   599 non-null    float64\n 18  (18,)   599 non-null    float64\n 19  (19,)   599 non-null    float64\n 20  (20,)   599 non-null    float64\n 21  (21,)   599 non-null    float64\n 22  (22,)   599 non-null    float64\n 23  (23,)   599 non-null    float64\n 24  (24,)   599 non-null    float64\n 25  (25,)   599 non-null    float64\n 26  (26,)   599 non-null    float64\n 27  (27,)   599 non-null    float64\n 28  (28,)   599 non-null    float64\n 29  (29,)   599 non-null    float64\n 30  (30,)   599 non-null    float64\n 31  (31,)   599 non-null    float64\n 32  (32,)   599 non-null    float64\n 33  (33,)   599 non-null    float64\n 34  (34,)   599 non-null    float64\n 35  (35,)   599 non-null    float64\n 36  (36,)   599 non-null    float64\n 37  (37,)   599 non-null    float64\n 38  (38,)   599 non-null    float64\n 39  (39,)   599 non-null    float64\n 40  (40,)   599 non-null    float64\n 41  (41,)   599 non-null    float64\n 42  (42,)   599 non-null    float64\n 43  (43,)   599 non-null    float64\n 44  (44,)   599 non-null    float64\n 45  (45,)   599 non-null    float64\n 46  (46,)   599 non-null    float64\n 47  (47,)   599 non-null    float64\n 48  (48,)   599 non-null    float64\n 49  (49,)   599 non-null    float64\n 50  (50,)   599 non-null    float64\n 51  (51,)   599 non-null    float64\n 52  (52,)   599 non-null    float64\n 53  (53,)   599 non-null    float64\n 54  (54,)   599 non-null    float64\n 55  (55,)   599 non-null    float64\n 56  (56,)   599 non-null    float64\n 57  (57,)   599 non-null    float64\n 58  (58,)   599 non-null    float64\n 59  (59,)   599 non-null    float64\n 60  (60,)   599 non-null    float64\n 61  (61,)   599 non-null    float64\n 62  (62,)   599 non-null    float64\n 63  (63,)   599 non-null    float64\n 64  (64,)   599 non-null    float64\n 65  (65,)   599 non-null    float64\n 66  (66,)   599 non-null    float64\n 67  (67,)   599 non-null    float64\n 68  (68,)   599 non-null    float64\n 69  (69,)   599 non-null    float64\n 70  (70,)   599 non-null    float64\n 71  (71,)   599 non-null    float64\n 72  (72,)   599 non-null    float64\n 73  (73,)   599 non-null    float64\n 74  (74,)   599 non-null    float64\n 75  (75,)   599 non-null    float64\n 76  (76,)   599 non-null    float64\n 77  (77,)   599 non-null    float64\n 78  (78,)   599 non-null    float64\n 79  (79,)   599 non-null    float64\n 80  (80,)   599 non-null    float64\n 81  (81,)   599 non-null    float64\n 82  (82,)   599 non-null    float64\n 83  (83,)   599 non-null    float64\n 84  (84,)   599 non-null    float64\n 85  (85,)   599 non-null    float64\n 86  (86,)   599 non-null    float64\n 87  (87,)   599 non-null    float64\n 88  (88,)   599 non-null    float64\n 89  (89,)   599 non-null    float64\n 90  (90,)   599 non-null    float64\n 91  (91,)   599 non-null    float64\n 92  (92,)   599 non-null    float64\n 93  (93,)   599 non-null    float64\n 94  (94,)   599 non-null    float64\n 95  (95,)   599 non-null    float64\n 96  (96,)   599 non-null    float64\n 97  (97,)   599 non-null    float64\n 98  (98,)   599 non-null    float64\n 99  (99,)   599 non-null    float64\ndtypes: float64(100)\nmemory usage: 468.1 KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's bring y to the required shape\n\ny_train = np.ravel(y)\nprint(y.shape,type(y), y_train.shape, type(y_train))\n\n#Data is full, no need delete outliers (NEED MORE Explanations)\nX_train = data\nX_test = dataT","execution_count":7,"outputs":[{"output_type":"stream","text":"(599, 1) <class 'pandas.core.frame.DataFrame'> (599,) <class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n### Will tune hyperparameters using GridSearchCV. For scoring will use area under the ROC curve: 'roc_auc'."},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For penalty will use Lasso 'l1'. Tune 'C' parameter\nparam_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n\nestimator = linear_model.LogisticRegression(solver='liblinear', penalty = 'l1', random_state = 1)\noptimizerL = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerL.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerL.best_score_)\nprint('param_opt', optimizerL.best_params_)","execution_count":8,"outputs":[{"output_type":"stream","text":"score_train_opt 0.9313434494237476\nparam_opt {'C': 0.5}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### RidgeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 2, 5]}\n\nestimator = linear_model.RidgeClassifier( random_state = 1)\noptimizerR = GridSearchCV(estimator, param_grid,  scoring = 'roc_auc',cv = 3)                    \noptimizerR.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerR.best_score_)\nprint('param_opt', optimizerR.best_params_)","execution_count":9,"outputs":[{"output_type":"stream","text":"score_train_opt 0.9032021454258864\nparam_opt {'alpha': 5}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier\nWe should have a loose stopping criterion and then use pruning to remove branches that contribute to overfitting. But pruning is a tradeoff between accuracy and generalizability, so our train scores might lower but the difference between train and test scores will also get lower.  This is what we need.  (details - https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_class = ensemble.RandomForestClassifier(random_state = 1)\ntrain_scores, test_scores = model_selection.validation_curve(rf_class, X_train, y_train, 'max_depth', list(range(1, 11)), cv=3, scoring='roc_auc')\nprint('max_depth=', list(range(1, 10)))\nprint(train_scores.mean(axis = 1))\nprint(test_scores.mean(axis = 1))","execution_count":10,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass param_name=max_depth, param_range=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"max_depth= [1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0.9556166  0.96918854 0.98051369 0.99123133 0.99765125 0.9995884\n 0.99995711 1.         1.         1.        ]\n[0.94072953 0.94530523 0.94503201 0.94783792 0.94701525 0.94778374\n 0.94746255 0.95092504 0.94865626 0.94697768]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We get the same difference between train and test scores on by  max_depth=4-9\nAnd we have the bigger score ROC AUC by max_depth=4"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': list(range(20, 100, 5)), 'min_weight_fraction_leaf': [0.001,  0.005, 0.01, 0.05, 0.1, 0.5] } \n\nestimator = ensemble.RandomForestClassifier(max_depth=4, random_state = 1)\noptimizerRF = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerRF.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerRF.best_score_)\nprint('param_opt', optimizerRF.best_params_)","execution_count":11,"outputs":[{"output_type":"stream","text":"score_train_opt 0.9513659591772741\nparam_opt {'min_weight_fraction_leaf': 0.001, 'n_estimators': 20}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Extreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth': list(range(1, 7)), 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1, 1.5], 'n_estimators': list(range(10, 100, 5)) }\nestimator = xgb.XGBClassifier( random_state = 1, min_child_weight=3)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_)","execution_count":12,"outputs":[{"output_type":"stream","text":"score_train_opt 0.9482604671053828\nparam_opt {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 60}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': list(range(10, 100, 5)), 'min_child_weight': list(range(1, 10)) }\nestimator = xgb.XGBClassifier( max_depth = 3, random_state = 1, learning_rate=0.1)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_) ","execution_count":13,"outputs":[{"output_type":"stream","text":"score_train_opt 0.9458848341935475\nparam_opt {'min_child_weight': 2, 'n_estimators': 45}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Will use the highest value ROC AUC model - RandomForestClassifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writting answers\n\nans=optimizerRF.predict(X_test)\n\nf=open(\"/kaggle/working/answers.csv\", \"w\")\nf.write(str(ans))\nf.close()\n","execution_count":14,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}